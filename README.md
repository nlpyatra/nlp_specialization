# Natural Language Processing Specialization

NLP set of courses from [Coursera](https://www.coursera.org/specializations/natural-language-processing).

## Course 1. Classification and Vector Spaces in NLP

**1. Logistic Regression for Sentiment Analysis of Tweets**

- Implement Logistic Regression from scratch
- Use LR model to classify positive or negative sentiment in tweets

**2. Naïve Bayes for Sentiment Analysis of Tweets**
- Implement Naive Bayes (NB) from scratch
- Use NB model for sentiment analysis

**3. Vector Space Models**

- Use vector space models to discover relationships between words and use principal component analysis (PCA) to reduce the dimensionality of the vector space and visualize those relationships

**4. Word Embeddings and Locality Sensitive Hashing for Machine Translation**

- Write a simple English-to-French translation algorithm using pre-computed word embeddings and locality sensitive hashing to relate words via approximate k-nearest neighbors search




## Course 2. Probabilistic Models in NLP

**1. Auto-correct using Minimum Edit Distance**
  - Vocabulary creation, text preprocessing, word counts
  - string manipulation: split, delete, swap, edit
  - Minumum edit distance: Dynamic programming
  - Create a simple auto-correct algorithm using minimum edit distance and dynamic programming

**2. Part-of-Speech (POS) Tagging**
  - Apply the Viterbi algorithm for POS tagging, which is important for computational linguistics

**3. N-gram Language Models**
  - Implement N-grams Corpus preprocessing
  - Implement building a language model: Probability matrix, Perplexity
  - Out of Vocabulary (OOV) words, Smooting, Back-off, interpolation
  - Write a better auto-complete algorithm using an N-gram model (similar models are used for translation, determining the author of a text, and speech recognition)

**4. Word2Vec and Stochastic Gradient Descent**
  - Word Embeddings: Tokenization, Sliding window of words, Transforming words to vectors, One-hot word vectors, Context word vectors
  - Continuous Bag of Words (CBOW) model : Implementation and training
  - Write your own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model



## Course 3. Sequence Models in NLP

**1. Sentiment with Neural Nets**

- Train a neural network with GLoVe word embeddings to perform sentiment analysis of tweets

**2. Language Generation Models**

- Generate synthetic Shakespeare text using a Gated Recurrent Unit (GRU) language model

**3. Named Entity Recognition (NER)**

- Train a recurrent neural network to perform NER using LSTMs with linear layers

**4. Siamese Networks**

- Use so-called ‘Siamese’ LSTM models to compare questions in a corpus and identify those that are worded differently but have the same meaning


## Course 4. Attention Models in NLP

**1. Neural Machine Translation with Attention**

- Translate complete English sentences into French using an encoder/decoder attention model

**2. Summarization with Transformer Models**

- Build a transformer model to summarize text

**3. Question-Answering with Transformer Models**

- Use T5 and BERT models to perform question answering

**4. Chatbots with a Reformer Model**

- Build a chatbot using a reformer model
